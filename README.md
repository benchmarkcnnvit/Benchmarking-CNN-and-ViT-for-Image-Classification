# Benchmarking Convolutional Neural Networks and Vision Transformers for Image Classification
The substantial growth in the application of CNNs and ViTs for image classification task has underscored the importance of benchmarking and conducting comprehensive evaluations. We benchmark the largest array of CNN and ViT models, extensively trained on the largest publicly available COVID-19 dataset to the best of our knowledge. We conducted a detailed comparison of the performance of CNN and ViT models on image classification task. This analysis, using chest radiographs of the lung, provides insight into the strengths and weaknesses of these different architectures. In the pursuit of identifying the most effective model, our study undertakes a meticulous analysis of both CNNs and ViTs, employing a diverse array of performance metrics. Beyond traditional accuracy measurements, the evaluation strategy incorporates computational efficiency metrics, including model run-time, CPU and GPU latency, and memory utilization. These metrics, often overlooked but critical in real-world applications, provide a holistic perspective on the feasibility of deploying these models in resource constrained environments. The analysis encompasses a vast spectrum of models, ranging from established CNN architectures like AlexNet and RegNet to cutting-edge ViT variants like Sep-ViT and Mobile-ViT.

<img width="580" alt="benchmark_process" src="https://github.com/benchmarkcnnvit/Benchmarking-CNN-and-ViT-for-Image-Classification/assets/158844533/68aae8cd-3f57-4492-90ea-41b5d3396449">
